{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-17T16:17:47.263771Z",
     "start_time": "2020-03-17T16:17:47.144276Z"
    }
   },
   "outputs": [],
   "source": [
    "cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# # Import Dask libraries\n",
    "\n",
    "# from dask import delayed\n",
    "# import joblib\n",
    "# import dask.dataframe as dd\n",
    "# import dask.array as da\n",
    "\n",
    "# from dask_ml.model_selection import train_test_split\n",
    "# from dask_ml.linear_model import LogisticRegression\n",
    "# from dask_ml.xgboost import XGBClassifier\n",
    "# from dask_ml.model_selection import RandomizedSearchCV\n",
    "# from dask_ml.model_selection import HyperbandSearchCV \n",
    "# # HyperbandSearchCVis Dask-MLâ€™s meta-estimator to find the best hyperparameters.\n",
    "# # It can be used as an alternative to RandomizedSearchCV to find similar hyper-parameters\n",
    "# # in less time by not wasting time on hyper-parameters that are not promising. \n",
    "\n",
    "from dask.distributed import Client, progress\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "\n",
    "client = Client(processes=False)\n",
    "# client = Client(processes=False, n_workers=4, threads_per_worker=8)\n",
    "client\n",
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-17T03:23:23.388071Z",
     "start_time": "2020-03-17T03:23:12.402438Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Tools\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Sampling & Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split, KFold \n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Visualizations\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load train and test sets\n",
    "# df            = pd.read_pickle('../data/df.pkl')\n",
    "X_train_under = pd.read_pickle('../data/03_X_train_under.pkl')\n",
    "X_test        = pd.read_pickle('../data/03_X_test.pkl')\n",
    "y_train_under = pd.read_pickle('../data/03_y_train_under.pkl')\n",
    "y_test        = pd.read_pickle('../data/03_y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Objective: Maximize F1 & ROC AUC Score as both recall and precision are equally important and the classes are imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Create list to store model performance\n",
    "model_performance = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "User RandomizedSearchCV instead:\n",
    "https://blog.usejournal.com/a-comparison-of-grid-search-and-randomized-search-using-scikit-learn-29823179bc85\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y actual\t: Counter({0: 83653, 1: 22301})\n",
      "y predicted\t: Counter({0: 105954})\n",
      "\n",
      "Confusion Matrix\n",
      "[[83653     0]\n",
      " [22301     0]]\n"
     ]
    }
   ],
   "source": [
    "#Dummy Classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy= 'most_frequent').fit(X_train_under, y_train_under)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#Distribution of y test\n",
    "print('y actual\\t:', Counter(y_test))\n",
    "\n",
    "#Distribution of y predicted\n",
    "print('y predicted\\t:', Counter(y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\nConfusion Matrix\\n' + str(confusion_matrix(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "base_models = []\n",
    "\n",
    "# Instantiate the models\n",
    "base_models.append(('LogisticRegression', LogisticRegression(solver='liblinear')))\n",
    "base_models.append(('KNeighbors', KNeighborsClassifier()))\n",
    "base_models.append(('RandomForest', RandomForestClassifier(n_estimators=10)))\n",
    "base_models.append(('XGBoost', XGBClassifier()))\n",
    "\n",
    "cv_results = []\n",
    "names = []\n",
    "\n",
    "# Cross Validate - 5 fold\n",
    "for name, model in base_models:\n",
    "    names.append(name)\n",
    "    cv_results.append(np.round_(cross_val_score(model, X_train_under, y_train_under, \n",
    "                                                cv=5, scoring='roc_auc', n_jobs=-1), 3))\n",
    "\n",
    "for i in range(len(names)):\n",
    "    print(names[i], round(cv_results[i].mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Grid searching key hyperparameters for logistic regression\n",
    "\n",
    "# Instantiate model and sampler\n",
    "lg_model = LogisticRegression()\n",
    "under = RandomUnderSampler(random_state=2020)\n",
    "\n",
    "# Construct pipeline\n",
    "steps = [('und', under), ('lgr', lg_model)]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Define parameter grid values to be searched\n",
    "param_grid = {\n",
    "    'und__random_state': [2020],\n",
    "    'lgr__solver': ['saga', 'liblinear'],\n",
    "    'lgr__penalty': ['l1', 'l2'],\n",
    "    'lgr__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "# Use stratify version of k-fold to keep class imbalance ratio\n",
    "k_fold = StratifiedKFold(n_splits=3, shuffle=True, random_state=2020)\n",
    "\n",
    "# Cross Validation\n",
    "# lg_grid = GridSearchCV(pipeline, param_grid=param_grid, cv=k_fold, n_jobs=-1, return_train_score=True, scoring='roc_auc')\n",
    "lg_rndm = RandomizedSearchCV(pipeline, param_distributions=param_grid, cv=k_fold, n_jobs=-1, return_train_score=True, scoring='f1_score')\n",
    "\n",
    "# Train with balanced classes\n",
    "grid_result = lg_rndm.fit(X_train_under, y_train_under)\n",
    "\n",
    "# Summarize results\n",
    "print(f'Best Score: {round(grid_result.best_score_, 3)}\\nParams: {grid_result.best_params_}\\n')\n",
    "\n",
    "mean_train = grid_result.cv_results_['mean_train_score']\n",
    "mean_test = grid_result.cv_results_['mean_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean_tr, mean_ts, param in zip(mean_train, mean_test, params):\n",
    "    print(f'Train {round(mean_tr, 3)}\\tTest {round(mean_ts, 3)}\\tParams: {param}')\n",
    "    \n",
    "# # Examine the best model\n",
    "# print(lg_grid.best_score_)\n",
    "# print(lg_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  : 0.74\n",
      "ROC_AUC   : 0.744\n",
      "Precision : 0.432\n",
      "Recall    : 0.75\n",
      "F-score   : 0.548\n",
      "\n",
      "Confusion Matrix\n",
      "[[61702 21951]\n",
      " [ 5584 16717]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with best paramaters\n",
    "lgr_best = LogisticRegression(C=1, penalty='l1', solver='liblinear', random_state=2020)\n",
    "\n",
    "# Train with balanced classes\n",
    "lgr_best.fit(X_train_under, y_train_under)\n",
    "\n",
    "# Get predictions\n",
    "lgr_best_preds = lgr_best.predict(X_test)\n",
    "# lgr_best_y_score = lgr_best.predict_proba(X_test)\n",
    "\n",
    "# Get ROC AUC Score, precision, recall, f1-score\n",
    "accuracy  = round(accuracy_score(y_test, lgr_best_preds), 3)\n",
    "roc_auc   = round(roc_auc_score(y_test, lgr_best_preds), 3)\n",
    "precision = round(precision_score(y_test, lgr_best_preds), 3)\n",
    "recall    = round(recall_score(y_test, lgr_best_preds), 3)\n",
    "f1        = round(f1_score(y_test, lgr_best_preds), 3)\n",
    "\n",
    "print(f'Accuracy  : {accuracy}')\n",
    "print(f'ROC_AUC   : {roc_auc}')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F-score   : {f1}')\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\nConfusion Matrix\\n' + str(confusion_matrix(y_test, lgr_best_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/lgr_best.sav']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add model and accuracy dict to list\n",
    "model_performance.append(dict([\n",
    "    ('Model', 'Logistic Regression'),\n",
    "    ('ROC AUC', round(roc_auc, 3)),\n",
    "    ('Precision', round(precision, 3)),\n",
    "    ('Recall', round(recall, 3)),\n",
    "    ('F1', round(f1, 3))\n",
    "     ]))\n",
    "\n",
    "# Save model for later use\n",
    "joblib.dump(lgr_best, '../models/lgr_best.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Grid searching key hyperparameters for KNN\n",
    "\n",
    "# Instantiate model and RandomUnderSampler\n",
    "knn_model = KNeighborsClassifier()\n",
    "under = RandomUnderSampler(random_state=2020)\n",
    "\n",
    "# Construct pipeline\n",
    "steps = [('und', under), ('knn', knn_model)]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Define parameter grid values to be searched\n",
    "param_grid = {\n",
    "    'und__random_state': [2020],\n",
    "    'knn__n_neighbors' : [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Use stratify version of k-fold to keep class imbalance ratio\n",
    "k_fold = StratifiedKFold(n_splits=3, shuffle=True, random_state=2020)\n",
    "\n",
    "# Cross Validation\n",
    "# knn_grid = GridSearchCV(pipeline, param_grid=param_grid, cv=k_fold, n_jobs=-1, return_train_score=True, scoring='roc_auc')\n",
    "knn_rndm = RandomizedSearchCV(pipeline, param_distributions=param_grid, cv=k_fold, n_jobs=-1, return_train_score=True, scoring='roc_auc')\n",
    "\n",
    "# Train with balanced classes\n",
    "grid_result = knn_rndm.fit(X_train_under, y_train_under) # Should I use X_train, y_train here?\n",
    "\n",
    "# Summarize results\n",
    "print(f'Best Score: {round(grid_result.best_score_, 3)}\\nParams: {grid_result.best_params_}\\n')\n",
    "\n",
    "mean_train = grid_result.cv_results_['mean_train_score']\n",
    "mean_test = grid_result.cv_results_['mean_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean_tr, mean_ts, param in zip(mean_train, mean_test, params):\n",
    "    print(f'Train {round(mean_tr, 3)}\\tTest {round(mean_ts, 3)}\\tParams: {param}')\n",
    "    \n",
    "# # Examine the best model\n",
    "# print(knn_grid.best_score_)\n",
    "# print(knn_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  : 0.706\n",
      "ROC_AUC   : 0.701\n",
      "Precision : 0.389\n",
      "Recall    : 0.691\n",
      "F-score   : 0.498\n",
      "\n",
      "Confusion Matrix\n",
      "[[59427 24226]\n",
      " [ 6882 15419]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with best paramaters\n",
    "knn_best = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# Train with balanced classes\n",
    "knn_best.fit(X_train_under, y_train_under)\n",
    "\n",
    "# Get predictions\n",
    "knn_best_preds = knn_best.predict(X_test)\n",
    "# knn_best_y_score = knn_best.predict_proba(X_test)\n",
    "\n",
    "# Get ROC AUC Score, precision, recall, f1-score\n",
    "accuracy  = round(accuracy_score(y_test,  knn_best_preds), 3)\n",
    "roc_auc   = round(roc_auc_score(y_test,   knn_best_preds), 3)\n",
    "precision = round(precision_score(y_test, knn_best_preds), 3)\n",
    "recall    = round(recall_score(y_test,    knn_best_preds), 3)\n",
    "f1        = round(f1_score(y_test,        knn_best_preds), 3)\n",
    "\n",
    "print(f'Accuracy  : {accuracy}')\n",
    "print(f'ROC_AUC   : {roc_auc}')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F-score   : {f1}')\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\nConfusion Matrix\\n' + str(confusion_matrix(y_test, knn_best_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/knn_best.sav']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add model and accuracy dict to list\n",
    "model_performance.append(dict([\n",
    "    ('Model', 'KNN'),\n",
    "    ('ROC AUC', round(roc_auc, 3)),\n",
    "    ('Precision', round(precision, 3)),\n",
    "    ('Recall', round(recall, 3)),\n",
    "    ('F1', round(f1, 3))\n",
    "     ]))\n",
    "\n",
    "# Save model for later use\n",
    "joblib.dump(knn_best, '../models/knn_best.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Grid searching key hyperparameters for Random Forest\n",
    "\n",
    "# Instantiate model and RandomUnderSampler\n",
    "rf_model = RandomForestClassifier()\n",
    "under = RandomUnderSampler(random_state=2020)\n",
    "\n",
    "# Construct pipeline\n",
    "steps = [('und', under), ('rfc', rf_model)]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Define parameter grid values to be searched\n",
    "param_grid = {\n",
    "    'und__random_state': [2020],\n",
    "    'rfc__n_estimators': [50, 100, 150, 200, 1000],\n",
    "    'rfc__max_depth' : [2, 3, 4],\n",
    "    'rfc__max_features' : [5, 10, 15],\n",
    "    'rfc__criterion' : ['gini', 'entropy'],\n",
    "    'rfc__random_state' :[2020]\n",
    "}\n",
    "\n",
    "# Use stratify version of k-fold to keep class imbalance ratio\n",
    "k_fold = StratifiedKFold(n_splits=3, shuffle=True, random_state=2020)\n",
    "\n",
    "# Cross Validation\n",
    "# rf_grid = GridSearchCV(pipeline, param_grid=param_grid, cv=k_fold, n_jobs=-1, return_train_score=True, scoring='roc_auc')\n",
    "rf_rndm = RandomizedSearchCV(pipeline, param_distributions=param_grid, cv=k_fold, n_jobs=-1, return_train_score=True, scoring='roc_auc')\n",
    "\n",
    "# Train with balanced classes\n",
    "grid_result = rf_rndm.fit(X_train_under, y_train_under) # Should I use X_train, y_train here?\n",
    "\n",
    "# Summarize results\n",
    "print(f'Best Score: {round(grid_result.best_score_, 3)}\\nParams: {grid_result.best_params_}\\n')\n",
    "\n",
    "mean_train = grid_result.cv_results_['mean_train_score']\n",
    "mean_test = grid_result.cv_results_['mean_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean_tr, mean_ts, param in zip(mean_train, mean_test, params):\n",
    "    print(f'Train {round(mean_tr, 3)}\\tTest {round(mean_ts, 3)}\\tParams: {param}')\n",
    "    \n",
    "# # Examine the best model\n",
    "# print(rf_grid.best_score_)\n",
    "# print(rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  : 0.745\n",
      "ROC_AUC   : 0.722\n",
      "Precision : 0.433\n",
      "Recall    : 0.68\n",
      "F-score   : 0.529\n",
      "\n",
      "Confusion Matrix\n",
      "[[63791 19862]\n",
      " [ 7126 15175]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with best paramaters\n",
    "rfc_best = RandomForestClassifier(n_estimators=1000, criterion='gini', max_depth=4, max_features=5, random_state=2020)\n",
    "\n",
    "# Train with balanced classes\n",
    "rfc_best.fit(X_train_under, y_train_under)\n",
    "\n",
    "# Get predictions\n",
    "rfc_best_preds = rfc_best.predict(X_test)\n",
    "# rfc_best_y_score = rfc_best.predict_proba(X_test)\n",
    "\n",
    "# Get ROC AUC Score, precision, recall, f1-score\n",
    "accuracy  = round(accuracy_score(y_test,  rfc_best_preds), 3)\n",
    "roc_auc   = round(roc_auc_score(y_test,   rfc_best_preds), 3)\n",
    "precision = round(precision_score(y_test, rfc_best_preds), 3)\n",
    "recall    = round(recall_score(y_test,    rfc_best_preds), 3)\n",
    "f1        = round(f1_score(y_test,        rfc_best_preds), 3)\n",
    "\n",
    "print(f'Accuracy  : {accuracy}')\n",
    "print(f'ROC_AUC   : {roc_auc}')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F-score   : {f1}')\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\nConfusion Matrix\\n' + str(confusion_matrix(y_test, rfc_best_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/rfc_best.sav']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add model and accuracy dict to list\n",
    "model_performance.append(dict([\n",
    "    ('Model', 'Random Forest'),\n",
    "    ('ROC AUC', round(roc_auc, 3)),\n",
    "    ('Precision', round(precision, 3)),\n",
    "    ('Recall', round(recall, 3)),\n",
    "    ('F1', round(f1, 3))\n",
    "     ]))\n",
    "\n",
    "# Save model for later use\n",
    "joblib.dump(rfc_best, '../models/rfc_best.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Grid searching key hyperparameters for XGBoost\n",
    "# Instantiate model and RandomUnderSampler\n",
    "xgb_model = XGBClassifier()\n",
    "under = RandomUnderSampler(random_state=2020)\n",
    "\n",
    "# Construct pipeline\n",
    "steps = [('und', under), ('xgb', xgb_model)]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Define parameter grid values to be searched\n",
    "param_grid = {\n",
    "    'und__random_state': [2020],\n",
    "    'xgb__n_estimators': [100, 250, 500, 1000], \n",
    "    'xgb__max_depth': [3, 4, 5], \n",
    "    'xgb__learning_rate': [0.01, 0.1] # 0.001\n",
    "}\n",
    "\n",
    "# Use stratify version of k-fold to keep class imbalance ratio\n",
    "k_fold = StratifiedKFold(n_splits=3, shuffle=True, random_state=2020)\n",
    "\n",
    "# Cross Validation\n",
    "# xgb_grid = GridSearchCV(pipeline, param_grid=param_grid, cv=k_fold, n_jobs=-1, return_train_score=True, scoring='roc_auc')\n",
    "xgb_rndm = RandomizedSearchCV(pipeline, param_distributions=param_grid, cv=3, n_jobs=-1, return_train_score=True, scoring='f1')\n",
    "# # Dask HyperbandSearchCV\n",
    "# search = HyperbandSearchCV(xgb_model, param_grid, max_iter=3, patience=True)\n",
    "\n",
    "# Train with balanced classes\n",
    "grid_result = xgb_rndm.fit(X_train_under, y_train_under)\n",
    "\n",
    "# Summarize results\n",
    "print(f'Best Score: {round(grid_result.best_score_, 3)}\\nParams: {grid_result.best_params_}\\n')\n",
    "\n",
    "mean_train = grid_result.cv_results_['mean_train_score']\n",
    "mean_test = grid_result.cv_results_['mean_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean_tr, mean_ts, param in zip(mean_train, mean_test, params):\n",
    "    print(f'Train {round(mean_tr, 3)}\\tTest {round(mean_ts, 3)}\\tParams: {param}')\n",
    "\n",
    "# # Examine the best model\n",
    "# print(lg_grid.best_score_)\n",
    "# print(lg_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  : 0.831\n",
      "ROC_AUC   : 0.822\n",
      "Precision : 0.571\n",
      "Recall    : 0.805\n",
      "F-score   : 0.668\n",
      "\n",
      "Confusion Matrix\n",
      "[[70146 13507]\n",
      " [ 4351 17950]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with best paramaters\n",
    "xgb_best = XGBClassifier(n_estimators=1000, max_depth=5, learning_rate=0.1)\n",
    "\n",
    "# Train with balanced classes\n",
    "xgb_best.fit(X_train_under, y_train_under)\n",
    "\n",
    "# Get predictions\n",
    "xgb_best_preds = xgb_best.predict(X_test)\n",
    "# xgb_best_y_score = xgb_best.predict_proba(X_test)\n",
    "\n",
    "# Get ROC AUC Score, precision, recall, f1-score\n",
    "accuracy  = round(accuracy_score(y_test,  xgb_best_preds), 3)\n",
    "roc_auc   = round(roc_auc_score(y_test,   xgb_best_preds), 3)\n",
    "precision = round(precision_score(y_test, xgb_best_preds), 3)\n",
    "recall    = round(recall_score(y_test,    xgb_best_preds), 3)\n",
    "f1        = round(f1_score(y_test,        xgb_best_preds), 3)\n",
    "\n",
    "print(f'Accuracy  : {accuracy}')\n",
    "print(f'ROC_AUC   : {roc_auc}')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F-score   : {f1}')\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\nConfusion Matrix\\n' + str(confusion_matrix(y_test, xgb_best_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Add model and accuracy dict to list\n",
    "model_performance.append(dict([\n",
    "    ('Model', 'XGBoost'),\n",
    "    ('ROC AUC', round(roc_auc, 3)),\n",
    "    ('Precision', round(precision, 3)),\n",
    "    ('Recall', round(recall, 3)),\n",
    "    ('F1', round(f1, 3))\n",
    "     ]))\n",
    "\n",
    "# Save model for later use\n",
    "joblib.dump(xgb_best, '../models/xgb_best.sav')\n",
    "\n",
    "# Save predictions for later use\n",
    "with open('../data/04_xgb_best_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_best_preds, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Pickel model performance\n",
    "with open('../data/04_model_performance.pkl', 'wb') as f:\n",
    "    pickle.dump(model_performance, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
